# Experiment Log

Goal is to organize our exploration, and figure out what avenues Madeline & Sidd will exploit, with markers for rough
time taken, links to result folders, brief summaries of experiments, etc. If you can link to commits with each 
"experiment", all the better,  but not a hard constraint (ideally we should be able to re-run at any point).

## [] Starting Point -- Reach 2D

**Guiding Question**: What does the playing field look like?
1. [Madeline] How does regular BC work at different amounts of data, generated by a simulated oracle?
2. [Madeline] How does vanilla DAgger work with the same simulated oracle, tying the data/model used by the initial
   policy at different amounts of data?
    + Evaluate with different "starting" policies (different initial data budgets)  
3. [Madeline] Visualize the policy actions across the 2D grid, for 10-100 different goals.
    + Take initial policy, DAgger policy at various iterations. Sample 10 goals, get 10 2D plots. Across the grid, plot
      the vector field (actions) and get an idea of robustness vs. distance from "oracle path". 
    + See here for example code: https://www.geeksforgeeks.org/how-to-plot-a-simple-vector-field-in-matplotlib/

**Important Points:**
- Preliminaries: We only care about one architecture (MLP) and BC / DAgger. We only care about the simulated expert
 (shortest path oracle). The only thing we're changing is the amount of data we receive from this oracle.
    + No Ensemble, no Linear model (although maybe just to sanity check if MLP doesn't learn).
    + Error bars w/ random seeds would be good. Keep data fixed.

- Hypothesis/What are we trying to do?
    + First step: Sanity check that our BC + DAgger code works. 
    + Second step: Additionally, we expect that for non-DAgger BC, robustness goes to shit as you quickly depart from
      the "oracle path". DAgger should be slightly more robust, but still goes to shit eventually.   

The following is from the original README... might need to be tweaked:

```
1. Generate data: ```./scripts/generate_reach2d_data/{desired experiment name here, see folder for options} ```
2. Train BC model: ```./scripts/reach2d_exps/bc_oracle.sh```
3. Training metrics and model checkpoints will be saved to `./out/{exp name here}`
```

## [] Initial Hypothesis -- What Makes a Bad Interaction?

*Hypothesis*: Interactive imitation learning fails to work when *interactions* (corrections, new demos, etc.) come from
a different distribution than is represented by the initial policy!
- This is under the assumption that (1) original policy is trained from demonstrations ~ *some optimal policy* and (2)
  the *interactions* are also sampled from *some other optimal policy*.
  
Example Experiments:
- [Madeline] Define a new environment -- Reach2D with Pillar. Goal is to reach from origin to a target point on the
  opposite side of the pillar.
- [Madeline] Define two policies - **Policy(under)** that provides demonstrations that **always go under** the pillar, 
  and Policy(over) that always tries to go above the pillar.
- [Madeline] Run BC on a mix of initial demos from Policy(under) (varying amounts), and then combined with demos from
  Policy(over) (various amounts).
- [Madeline] Run DAgger using Policy(under) as the interaction policy, starting with a policy initialized from 
  Policy(over) - should vary the number of demonstrations.
  
Here's what we might expect: 
- With equal amounts of Policy(under) and Policy(over) data, the policy ends up not as great as if it were trained
  from a single policy alone.
    - This should be easily confirmed (in the BC setting)
- With *small amounts of initial data*, the interventions don't matter as much (general uncertainty in base policy).
- With larger amounts of initial data, the interventions (for DAgger) end up hurting performance.

What does this tell us?
- We need some way of understanding "incompatibility" between an existing policy, and a demonstrator.
    + Question: How do we parameterize the demonstrator? Some arbitrary policy we can take trajectory samples from? Or
      that we can query at individual states?
- We need some way of measuring incompatibility *subject to* confidence of our existing policy (and potentially our 
  demonstrator). If our policy has no idea what to do, then *any confident demonstrator* is compatible!

## [] Developing a Measure of Incompatibility

We're going to divide this into steps -- assume a lot at first, and try and get at the "perfect" measure of
incompatibility that we'd want. Then we're going to iteratively back off and relax assumptions until we're at our
current problem setting.

#### Setting 1 :: Incompatibility as Difference between Reward Functions

Assumptions:
- Let's assume that all reward functions can be expressed as some linear combination of hand-written features. For
  example these features could be distance to goal, distance to pillar, 1 if under pillar, 0 if above pillar, etc.
  
- Let's assume we have the prerequisites necessary to run MaxEnt Inverse Reinforcement Learning (or Bayesian IRL) for our
 given environment.
    + Resources for MaxEnt IRL w/ Code: https://github.com/qzed/irl-maxent
    + Resources for Bayesian IRL w/ Links to Code: https://silviatulli.com/2021/01/29/bayesian-inverse-reinforcement-learning/

**Punchline**: IRL gives us the ability to extract a set of weights $w$ for each feature (and therefore a reward
 function) given a set of demonstrations.
- Given above, one way to measure incompatibility: $||w_initial - w_demonstrator||^2$ after running IRL on 10 demos
  sampled from each!
- But... how do we handle uncertainty of $pi_initial$? 
    + **Question for Dorsa about uncertainty & MaxEnt IRL!**
    + Worst-case, assume $\eps$ reflects some score of uncertainty for each of $w_initial$, $w_demonstrator$. One way to
      do this (idk how principled) is to do leave-one-out IRL with 9/10 of the collected demos, and take mean/std of 
      corresponding weight vectors.
        + Uncertainty-adjusted incompatibility: (eps_demonstrator / eps_initial) * ||w_initial - w_demonstrator||^2$. 
    
Experiments:
- [Madeline] Pick the settings for Reach2D Pillar from above, and compute the incompatibility scores for each, using the
  IRL procedure above.
- [Madeline] Validate results.

What does this tell us/what should we expect:
- Incompatibility score for "good" policy trained from Policy(under) w/ demonstrator Policy(over) should be high!
- Incompatibility score for any shitty policy and a good demonstrator should be low.
    + Corollary: Incompatibility for shitty policy & shitty demonstrator should be **IDK - @Madeline up to you!**
- Incompatibility score for "good" policy and aligned demonstrator should be low!

#### Setting 2 :: Incompatibility as Likelihood under Probabilistic Policy

It's very rare that we can express all the features/preferences that a human has a priori -- even so, in larger state
spaces, running IRL multiple times like this isn't tractable. Instead, we want some measure of incompatibility that is
reward independent.

What is a good proxy: predictability under the initial policy! If actions by a demonstrator are predictable under the
base policy, then there is a compatibility. If not, incompatible!
    + Note that the policy inherent uncertainty isn't covered here... bad metric!

Experiments:
- [Sidd] Rewrite the MLP to be a probabilistic regressor (output Gaussian mean, variance), maximize log-likelihood of
  data. 
    + Could also work for Ensemble, Dropout-Ensemble... punt on this for a while.
- [Sidd] Run Validation Experiments from Setting 1 (from Madeline)
     

#### Setting 3 :: Entropy-Aware Likelihood under Probabilitic Policies, Ensembles, and Dropout Ensembles

[Sidd/Madeline]: To be discussed, we probably want something like Bayesian Active Learning by Disagreement... something
that explicitly allows us to account for issues/miscalibration/errors in the base policy. 


## [] Nudging Users towards Compatibility

Given our measure of incompatibility... how do we take a first step towards nudging demonstrators towards providing
"better" demonstrations.

**Assumptions**: We need to be really careful here...
- All "potential" demonstrations come from an optimal policy (@Sidd: Would like to eliminate this if possible...).
- We can reset the same task multiple times, and run with different policies.

Experiment:
- [Sidd] Sample k-different reward functions for the Reach2D-Pillar Environment. Choose 1 as the initial policy to draw
  demonstrations from, and train base policy.
    + [Sidd] Run RL to train k-different expert policies for the different reward functions.
- [Madeline] Given a new "data collection" task, run out the various policies in a pairwise fashion, using the relative
  incompatibility scores to pick the "best" policy to collect data for.
    + Can either be an absolute ordering (pick best policy), or truncated.
    + Run this procedure out every few iterations.
- [Madeline] Compare this with the following baselines:
    + Collect data with known bad policy (should beat always)
    + Collect data with mixture of policies (sample randomly every iteration)
    + Collect data with mixture of "good-enough" policies (filter out lowest incompatibility)
    + Collect data with optimal (same as initial) policy. 

## [] Expanding Incompatibility to Robosuite

TODO: To be filled out by Sidd after "Nudging" results. Just a scaled up version of the incompatibility/reward
experiments with lots of reward functions, on non-trivial tasks.

## [] Fleshing out Baselines & Extra Simulated Experiments

TODO: To be filled out by Madeline after "Nudging" results.

## [] Fleshing out Real-World User Study

TODO: To be filled out by Madeline after "Nudging" results.

